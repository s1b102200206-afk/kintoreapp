# app.py
import streamlit as st
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFont

st.title("スクワット姿勢解析アプリ")

# --- モード選択 ---
mode = st.selectbox("スクワット解析モードを選んでください", ["shallow", "deep"])

# --- 動画アップロード ---
uploaded_file = st.file_uploader("解析したい動画をアップロード", type=["mp4"])
if uploaded_file is not None:
    video_path = uploaded_file.name
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    st.info("動画を解析中です…少しお待ちください")

    # --- MoveNetモデル読み込み ---
    model = hub.load("https://tfhub.dev/google/movenet/singlepose/lightning/4")
    movenet = model.signatures['serving_default']

    # --- 関数定義 ---
    def calculate_angle(a, b):
        a, b = np.array(a), np.array(b)
        vertical = np.array([0, -1])
        spine = a - b
        cosine_angle = np.dot(spine, vertical) / (np.linalg.norm(spine) * np.linalg.norm(vertical) + 1e-6)
        return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))

    def analyze_frame(frame, mode="shallow"):
        orig = frame.copy()
        img_resized = tf.image.resize_with_pad(tf.convert_to_tensor(frame), 192, 192)
        input_img = tf.expand_dims(tf.cast(img_resized, dtype=tf.int32), axis=0)
        keypoints = movenet(input_img)['output_0'].numpy()[0,0,:,:]
        h, w, _ = orig.shape

        points = {}
        kp_idx = {"left_shoulder":5,"right_shoulder":6,"left_hip":11,"right_hip":12,
                  "left_knee":13,"right_knee":14,"left_ankle":15,"right_ankle":16}
        for name, idx in kp_idx.items():
            points[name] = (keypoints[idx][1]*w, keypoints[idx][0]*h, keypoints[idx][2])
        conf_thresh = 0.1

        # 膝角度
        def angle(a,b,c):
            a,b,c = np.array(a[:2]), np.array(b[:2]), np.array(c[:2])
            ba, bc = a-b, c-b
            return np.degrees(np.arccos(np.clip(np.dot(ba,bc)/(np.linalg.norm(ba)*np.linalg.norm(bc)+1e-6), -1.0, 1.0)))
        knee_angle = angle(points["left_hip"], points["left_knee"], points["left_ankle"])
        mid_shoulder = ((points["left_shoulder"][0]+points["right_shoulder"][0])/2,
                        (points["left_shoulder"][1]+points["right_shoulder"][1])/2)
        mid_hip = ((points["left_hip"][0]+points["right_hip"][0])/2,
                   (points["left_hip"][1]+points["right_hip"][1])/2)
        back_angle = calculate_angle(mid_shoulder, mid_hip)

        # コメント生成
        if mode=="shallow":
            knee_comment = "浅めOK" if knee_angle>100 else "少し浅め" if knee_angle>70 else "しゃがみすぎ注意"
        else:
            knee_comment = "深めOK" if knee_angle<80 else "もう少し深く" if knee_angle<100 else "浅すぎ"
        back_comment = "背中まっすぐ" if back_angle<15 else f"背中曲がり({int(back_angle)}°)"

        # 関節描画
        for pt in points.values():
            if pt[2]>conf_thresh: cv2.circle(orig, tuple(map(int, pt[:2])), 5, (0,255,0), -1)
        bones = [("left_shoulder","left_hip"),("right_shoulder","right_hip"),
                 ("left_hip","left_knee"),("left_knee","left_ankle"),
                 ("right_hip","right_knee"),("right_knee","right_ankle")]
        for a,b in bones:
            if points[a][2]>conf_thresh and points[b][2]>conf_thresh:
                cv2.line(orig, tuple(map(int, points[a][:2])), tuple(map(int, points[b][:2])), (255,0,0), 2)
        cv2.line(orig, tuple(map(int, mid_shoulder)), tuple(map(int, mid_hip)), (0,0,255), 2)

        return orig, (knee_comment, back_comment)

    # --- 動画処理 ---
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)/2
    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    w_new = 320
    h_new = int(orig_h * w_new / orig_w)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    output_path = "squat_analysis_small.mp4"
    out = cv2.VideoWriter(output_path, fourcc, fps, (w_new,h_new))

    font_path = "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"
    font = ImageFont.truetype(font_path, 20)

    while True:
        ret, frame = cap.read()
        if not ret: break
        frame = cv2.resize(frame, (w_new, h_new))
        result_img, comments = analyze_frame(frame, mode)
        knee_comment, back_comment = comments
        img_pil = Image.fromarray(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        draw.text((10,10), f"下半身: {knee_comment}", fill=(0,0,255), font=font)
        draw.text((10,40), f"上半身: {back_comment}", fill=(255,0,0), font=font)
        out.write(cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR))

    cap.release()
    out.release()

    st.success("動画解析完了！")
    st.video(output_path)
    st.write(f"下半身: {knee_comment}")
    st.write(f"上半身: {back_comment}")
